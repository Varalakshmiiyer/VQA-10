{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Import Libraries for General Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Softwares/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg') \n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "import ujson as json\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# look at dataset\n",
    "from imp import reload\n",
    "import image_grid\n",
    "reload(image_grid)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries for Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import spacy.en\n",
    "#from spacy.strings import StringStore, hash_string\n",
    "\n",
    "import snowballstemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "\n",
    "# preparing data for gpu\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries for Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer, ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading VQA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reading_vqa_data(vqa_dir, section):\n",
    "    ans = 'mscoco_%s2014_annotations.json' % section\n",
    "    with (vqa_dir / ans).open() as file_:\n",
    "        ans_data = json.load(file_)\n",
    "    image_by_id = {}\n",
    "    answers_by_id = {}\n",
    "    for answer in ans_data['annotations']:\n",
    "        image = str(answer['image_id'])\n",
    "        mca = answer['multiple_choice_answer']\n",
    "        img = '0'*(12 - len(image)) + image\n",
    "        s = '/data/%s/images' % section\n",
    "        s = s + '/COCO_%s2014_' % section + img + '.jpg'\n",
    "        image_by_id[answer['question_id']] = s\n",
    "        answers_by_id[answer['question_id']] = mca\n",
    "    filename = ('MultipleChoice_mscoco_'\n",
    "                '%s2014_questions.json' % section)\n",
    "    with (vqa_dir / filename).open() as file_:\n",
    "        ques_data = json.load(file_)\n",
    "    for question in ques_data['questions']:\n",
    "        text = question['question']\n",
    "        ques_id = question['question_id']\n",
    "        options = question['multiple_choices']\n",
    "        image_path = image_by_id[ques_id]\n",
    "        image = Image.open(image_path)\n",
    "        if min(image.size) < IMAGE_SIZE:\n",
    "            image_path = prev_image\n",
    "            image_by_id[ques_id] = image_path\n",
    "        else:\n",
    "            if (answers_by_id[ques_id] == 'yes'):\n",
    "                prev_image = image_path\n",
    "        yield ques_id, image_by_id[ques_id], text, options, answers_by_id[ques_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "image_thres = 200\n",
    "train_data = list(reading_vqa_data(Path('/data/train'), 'train'))\n",
    "eval_data = list(reading_vqa_data(Path('/data/val'), 'val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_answers(data, top_n=1000):\n",
    "    freqs = Counter()\n",
    "    ans2id = {}\n",
    "    id2ans = {}\n",
    "    ans2ques = {}\n",
    "    id2ques = {}\n",
    "    for ques_id, _, _2, _3, answer in data:\n",
    "        freqs[answer] += 1\n",
    "        ans2ques[answer] = ques_id\n",
    "    most_common = freqs.most_common(top_n)\n",
    "    #most_common = freqs.most_common()\n",
    "    for i, (string, _) in enumerate(most_common):\n",
    "        ans2id[string] = i+1\n",
    "        id2ans[i+1] = string\n",
    "        id2ques[i+1] = ans2ques[string]\n",
    "    return ans2id, id2ans, id2ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_n = 16140\n",
    "ans2id, id2ans, id2ques = get_answers(train_data, top_n)\n",
    "most_common_a = id2ans[1]\n",
    "most_common_q = id2ques[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_answers(data):\n",
    "    image_files = {}\n",
    "    for ques_id, image_path, text, opt, answer in data:\n",
    "        #if exclude_missing and answer not in answers:\n",
    "        if answer not in ans2id:\n",
    "            ans2id[answer] = 0\n",
    "            id2ans[0] = most_common_a\n",
    "            id2ques[0] = most_common_q\n",
    "        idx = ans2id[answer]\n",
    "        image_paths = image_files.get(idx)\n",
    "        if (image_paths == None):\n",
    "            image_paths = []\n",
    "        image_paths.append((image_path))\n",
    "        #if (len(image_paths) < image_thres):\n",
    "            #image_paths.append((image_path))\n",
    "        image_files[idx] = image_paths\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_answers(data, exclude_missing=False):\n",
    "    encoded = []\n",
    "    for ques_id, image, text, opt_ans, answer in data:\n",
    "        for a in opt_ans:\n",
    "            if a not in ans2id:\n",
    "                ans2id[a] = 0\n",
    "                id2ans[0] = most_common_a\n",
    "                id2ques[0] = most_common_q\n",
    "        opt = [ans2id[a] for a in opt_ans]\n",
    "        encoded.append((ques_id, image, text, opt_ans, opt, answer, ans2id.get(answer, 0)))\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_images = image_answers(train_data)\n",
    "train_data = encode_answers(train_data)\n",
    "eval_images = image_answers(eval_data)\n",
    "eval_data = encode_answers(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = 0\n",
    "n_eval = 0\n",
    "for answer in training_images:\n",
    "    n_train = n_train + len(training_images[answer])\n",
    "for answer in eval_images:\n",
    "    n_eval = n_eval + len(eval_images[answer])\n",
    "\n",
    "print(n_train, n_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_image(image, true_size, grayscale=True):\n",
    "    factor = min(image.size) / float(true_size)\n",
    "    #if min(image.size) < true_size:\n",
    "    #    raise ValueError('image to small, is {}, '\n",
    "    #                     'needs to be min: {}'.format(min(image.size), true_size))\n",
    "    image.thumbnail(np.ceil(np.array(image.size) / factor))\n",
    "    image_narray = np.array(image)\n",
    "    if grayscale:\n",
    "        if image_narray.ndim == 3:\n",
    "            image_narray = image_narray.mean(axis=2)\n",
    "        width, height = image_narray.shape\n",
    "    else:\n",
    "        if image_narray.ndim == 2:\n",
    "            image_narray = np.dstack([image_narray, image_narray, image_narray])\n",
    "        width, height, _ = image_narray.shape\n",
    "    offset_width = (width - true_size) / 2\n",
    "    offset_height = (height - true_size) / 2\n",
    "    image_narray = image_narray[offset_width:offset_width+true_size, offset_height:offset_height+true_size]\n",
    "    if grayscale:\n",
    "        if image_narray.shape != (true_size, true_size):\n",
    "            raise ValueError('Preparing failed. '\n",
    "                             'Image should be {}, but is'\n",
    "                             '{}'.format(image_narray.shape,[true_size, true_size, 3] ))\n",
    "    else:\n",
    "        if image_narray.shape != (true_size, true_size, 3):\n",
    "            raise ValueError('Preparing failed. '\n",
    "                             'Image should be {}, but is'\n",
    "                             '{}'.format(image_narray.shape,[true_size, true_size, 3] ))\n",
    "\n",
    "    return image_narray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16141, 5413)\n"
     ]
    }
   ],
   "source": [
    "print(len(training_images), len(eval_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248349, 121512)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16141, 5413)\n"
     ]
    }
   ],
   "source": [
    "print(len(training_images), len(eval_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and process tofeatures\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "dim_features = IMAGE_SIZE**2\n",
    "n_train_features = n_train\n",
    "train_features = np.ones([n_train_features, dim_features]) * np.nan\n",
    "train_labels = np.ones([n_train_features]) * np.nan\n",
    "\n",
    "n_test_features = n_eval\n",
    "test_features = np.ones([n_test_features, dim_features]) * np.nan\n",
    "test_labels = np.ones([n_test_features]) * np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "for answer_index, answer in enumerate(training_images):\n",
    "    image_paths = training_images[answer]\n",
    "    for feature_file in image_paths:\n",
    "        image = Image.open(feature_file)\n",
    "        feature_data = prepare_image(image, IMAGE_SIZE).reshape(dim_features)    \n",
    "        train_features[index] = feature_data\n",
    "        train_labels[index] = answer\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "for answer_index, answer in enumerate(eval_images):\n",
    "    image_paths = eval_images[answer]\n",
    "    for feature_file in image_paths:\n",
    "        image = Image.open(feature_file)\n",
    "        feature_data = prepare_image(image, IMAGE_SIZE).reshape(dim_features)     \n",
    "        test_features[index] = feature_data\n",
    "        test_labels[index] = answer\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_array(array_to_check):\n",
    "    if np.any(np.isnan(array_to_check)):\n",
    "        raise ValueError('Nan found')\n",
    "\n",
    "check_array(train_features)\n",
    "check_array(train_labels)\n",
    "check_array(test_features)\n",
    "check_array(test_labels)\n",
    "train_labels = np.array(train_labels, dtype=np.int)\n",
    "test_labels = np.array(test_labels, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.50612379053227696, 0.58939326291218341, -2.5669579524208565e-15)\n",
      "(-0.50612379053227696, 0.58939326291218341, -0.0020737482711646999)\n"
     ]
    }
   ],
   "source": [
    "# normalizing features\n",
    "\n",
    "def norm_features(train_features, test_features):\n",
    "    max_array = train_features.max(0)\n",
    "    train_features_0_1 =train_features / max_array\n",
    "    test_features_0_1 = test_features / max_array\n",
    "    mean_array = train_features_0_1.mean(0)\n",
    "    train_features_normed = train_features_0_1 - mean_array\n",
    "    test_features_normed = test_features_0_1 - mean_array\n",
    "    return train_features_normed, test_features_normed\n",
    "train_features_normed, test_features_normed = norm_features(train_features, test_features)\n",
    "\n",
    "print(train_features_normed.min(), train_features_normed.max(), train_features_normed.mean())\n",
    "print(test_features_normed.min(), test_features_normed.max(), test_features_normed.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((248349, 4096), 248349, (121512, 4096), 121512)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_normed.shape, len(train_labels), test_features_normed.shape, len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_shared_GPU(data):\n",
    "    \"\"\"Place the data into shared variables.  This allows Theano to copy\n",
    "    the data to the GPU, if one is available.\n",
    "\n",
    "    \"\"\"\n",
    "    shared_x = theano.shared(\n",
    "        np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
    "    shared_y = theano.shared(\n",
    "        np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
    "    return shared_x, T.cast(shared_y, \"int32\")\n",
    "    #return shared_x, T.cast(shared_y, \"int64\")\n",
    "training_data = make_shared_GPU([train_features_normed, train_labels])\n",
    "validation_data = make_shared_GPU([test_features_normed, test_labels])\n",
    "test_data = validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((248349, 4096), 248349, (121512, 4096), 121512)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_normed.shape, len(train_labels), test_features_normed.shape, len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "9.68911789777\n"
     ]
    }
   ],
   "source": [
    "## adding a conv layer\n",
    "#THEANO_FLAGS=\"exception_verbosity=high\"\n",
    "mini_batch_size = 83\n",
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 64, 64), \n",
    "                      filter_shape=(20, 1, 5, 5), \n",
    "                      poolsize=(2, 2),\n",
    "                      activation_fn=ReLU),\n",
    "        FullyConnectedLayer(n_in=20*30*30, n_out=100, activation_fn=ReLU),\n",
    "        SoftmaxLayer(n_in=100, n_out=16141)], mini_batch_size)\n",
    "net.SGD(training_data, 60, mini_batch_size, 0.1, \n",
    "            validation_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
